{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["### Tuning a Full-Scale Connected Skip Connections U-Net with DARTs\n","This notebook focuses on tuning the skip connections (in particular) between the encoding and decoding branches. These skip connections have a large impact on the final preformance of the model. Evaluated on the HAM10000 dataset."],"metadata":{"id":"L4L03N9dTLEv"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"B5QEgP-bTG-j","executionInfo":{"status":"ok","timestamp":1715413623756,"user_tz":420,"elapsed":1129,"user":{"displayName":"Christian Granados","userId":"18308979293956140005"}}},"outputs":[],"source":["import warnings\n","warnings.simplefilter(action='ignore', category=FutureWarning)\n","from google.colab.patches import cv2_imshow\n","import matplotlib.pyplot as plt\n","from collections import namedtuple\n","from skimage.color import rgb2gray\n","import matplotlib.pyplot as plt\n","from google.colab import files\n","from datetime import datetime\n","from scipy.stats import norm\n","from skimage import io\n","import seaborn as sns\n","import pandas as pd\n","import numpy as np\n","import logging\n","import random\n","import string\n","import heapq\n","import glob\n","import time\n","import math\n","import cv2\n","import sys\n","import ast\n","import os\n","import re"]},{"cell_type":"code","source":["from torch.utils.data import Dataset, DataLoader\n","from skimage import data, transform, util\n","import torchvision.transforms as transforms\n","from torchsummary import torchsummary\n","from skimage.transform import resize\n","from torch.autograd import Variable\n","import torch.nn.functional as F\n","import torch.nn as nn\n","import torchvision\n","import torch"],"metadata":{"id":"aJ2A-AQXUrUv","executionInfo":{"status":"ok","timestamp":1715413626980,"user_tz":420,"elapsed":3225,"user":{"displayName":"Christian Granados","userId":"18308979293956140005"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WgP6Y9qHUTzc","executionInfo":{"status":"ok","timestamp":1715413627772,"user_tz":420,"elapsed":794,"user":{"displayName":"Christian Granados","userId":"18308979293956140005"}},"outputId":"f7aef802-deb4-4c7a-b0aa-253a93ec2a46"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KWlJCd3sJGkk","executionInfo":{"status":"ok","timestamp":1715413627772,"user_tz":420,"elapsed":5,"user":{"displayName":"Christian Granados","userId":"18308979293956140005"}},"outputId":"d77cb097-572e-4efa-d5a7-b2ec0fe18387"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda:0\n"]}]},{"cell_type":"markdown","source":["# 1. Data"],"metadata":{"id":"cJVLrbKUxjd7"}},{"cell_type":"code","source":["class HAM10000_Dataset(Dataset):\n","\n","  def __init__(self, csv_file, num_entries = 10000):\n","    self.num_entries = num_entries\n","    self.data = pd.read_csv(csv_file, usecols = [ 1, 2])\n","    assert self.num_entries <= len(self.data)\n","\n","  def __len__(self):\n","    return self.num_entries\n","\n","  def __getitem__(self, index):\n","    ind = index if index < (self.num_entries // 2) else index + (len(self.data) - self.num_entries) // 2\n","\n","    downscale = lambda x: F.interpolate(x, size = (128, 128), mode = 'bilinear', align_corners = True)\n","\n","    img = io.imread(self.data.iloc[ind, 0])\n","    img_tensor = downscale(transforms.ToTensor()(img).unsqueeze(0)).squeeze(0)\n","\n","    pmask = (downscale(transforms.ToTensor()(rgb2gray(io.imread(self.data.iloc[ind, 1]))).unsqueeze(0))).bool().squeeze(0)\n","    mask_tensor = torch.cat((pmask.float(), (~pmask).float()), dim = 0)\n","\n","    return (img_tensor.float(), mask_tensor)"],"metadata":{"id":"rnxB5zZpxlZv","executionInfo":{"status":"ok","timestamp":1715413627772,"user_tz":420,"elapsed":3,"user":{"displayName":"Christian Granados","userId":"18308979293956140005"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["# 2. Operations"],"metadata":{"id":"Hk6T5dzjVXWv"}},{"cell_type":"code","source":["class DownSampl(nn.Module):\n","    '''\n","    Bilinear downscale for images\n","    '''\n","    def __init__(self, d_img_in, d_img_out):\n","        super(DownSampl, self).__init__()\n","        self.op = lambda x: F.interpolate(\n","            x,\n","            size = (d_img_out, d_img_out),\n","            mode = 'bilinear',\n","            align_corners = True\n","            )\n","\n","    def forward(self, x):\n","        return self.op(x)"],"metadata":{"id":"yo4LxiMj9NWA","executionInfo":{"status":"ok","timestamp":1715413627772,"user_tz":420,"elapsed":3,"user":{"displayName":"Christian Granados","userId":"18308979293956140005"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["class UpSampl(nn.Module):\n","\n","    def __init__(self, d_img_in, d_img_out):\n","        '''\n","        Bilinear upscale for images\n","        '''\n","        super(UpSampl, self).__init__()\n","        scale = d_img_out // d_img_in\n","        self.op = torch.nn.Upsample(scale_factor = scale, mode = 'bilinear')\n","\n","    def forward(self, x):\n","        return self.op(x)"],"metadata":{"id":"qIbaorI39O4h","executionInfo":{"status":"ok","timestamp":1715413627772,"user_tz":420,"elapsed":3,"user":{"displayName":"Christian Granados","userId":"18308979293956140005"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["class Zero(nn.Module):\n","\n","  def __init__(self, d_out, stride):\n","    super(Zero, self).__init__()\n","    self.stride = stride\n","    self.d_out = d_out\n","\n","  def forward(self, x):\n","    if self.stride == 1:\n","      return torch.cat([x.mul(0.)[:, :1, :, :] for _ in range(self.d_out)], dim = 1)\n","    return torch.cat([x[:,:,::self.stride,::self.stride].mul(0.)[:, :1, :, :] for _ in range(self.d_out)], dim = 1)"],"metadata":{"id":"tOerIwoQ2jjh","executionInfo":{"status":"ok","timestamp":1715413627772,"user_tz":420,"elapsed":3,"user":{"displayName":"Christian Granados","userId":"18308979293956140005"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["class SkipConnection(nn.Module):\n","\n","  def __init__(self, d_in, d_out):\n","    super(SkipConnection, self).__init__()\n","    self.d_in = d_in\n","    self.d_out = d_out\n","\n","  def forward(self, x):\n","    if self.d_in == self.d_out:\n","        return x\n","\n","    elif self.d_in < self.d_out:\n","        seq = np.array([[i for _ in range(self.d_out // self.d_in)] for i in range(self.d_in)]).flatten()\n","        return torch.cat(\n","            [x[:, i:i+1, :, :] for i in seq],\n","            dim = 1\n","            )\n","\n","    else:\n","        glen = self.d_in // self.d_out\n","        groups = torch.split(x, glen, dim=1)\n","        op1 = nn.AvgPool3d(kernel_size=(glen, 1, 1), stride=(1, 1, 1), padding=(0, 0, 0)).cuda()\n","        out = []\n","        for g in groups:\n","            out.append(op1(g))\n","        return torch.cat(out, dim=1)"],"metadata":{"id":"wFbfDea-Vdqv","executionInfo":{"status":"ok","timestamp":1715413627772,"user_tz":420,"elapsed":3,"user":{"displayName":"Christian Granados","userId":"18308979293956140005"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["class MaxPoolBN(nn.Module):\n","\n","  def __init__(self, d_in, d_out, stride):\n","    super(MaxPoolBN, self).__init__()\n","    self.op = nn.Sequential(\n","      nn.MaxPool2d(3, stride = stride, padding = 1),\n","      nn.BatchNorm2d(d_in, affine = False)\n","    )\n","\n","    self.d_out = d_out\n","    self.d_in = d_in\n","\n","  def forward(self, x):\n","    '''\n","    MaxPoolBN cannot adjust the thickenss of the layers because it is a filter\n","    slid over each layer, we need to account for d_in != d_out. What makes this\n","    easier is they they are all factors of 2 larger or smaller than eachother.\n","\n","    If d_out > d_in, we duplicate in the input image's channels until we reach\n","    the dimensionality of d_out. If d-in is larger than d_out, we apply a\n","    3D Max Pool groupwise to the input image to reduce dimensionality.\n","    '''\n","    out = self.op(x)\n","\n","    if self.d_out > self.d_in:\n","      seq = np.array([[i for _ in range(self.d_out // self.d_in)] for i in range(self.d_in)]).flatten()\n","      return torch.cat(\n","          [out[:, i:i+1, :, :] for i in seq],\n","          dim = 1\n","          )\n","\n","    elif self.d_out < self.d_in:\n","        glen = self.d_in // self.d_out\n","        groups = torch.split(x, glen, dim=1)\n","        op1 = nn.MaxPool3d(kernel_size=(glen, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1)).cuda()\n","        out = []\n","        for g in groups:\n","            out.append(op1(g))\n","        out = torch.cat(out, dim=1)\n","        bn = nn.BatchNorm2d(self.d_out, affine = False).cuda()\n","        return bn(out)\n","\n","    return out\n"],"metadata":{"id":"NzuCrLKa2Zv-","executionInfo":{"status":"ok","timestamp":1715413627772,"user_tz":420,"elapsed":3,"user":{"displayName":"Christian Granados","userId":"18308979293956140005"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["class AvgPoolBN(nn.Module):\n","\n","  def __init__(self, d_in, d_out, stride):\n","    super(AvgPoolBN, self).__init__()\n","    self.op = nn.Sequential(\n","      nn.AvgPool2d(3, stride = stride, padding = 1),\n","      nn.BatchNorm2d(d_in, affine = False)\n","    )\n","\n","    self.d_out = d_out\n","    self.d_in = d_in\n","\n","  def forward(self, x):\n","    out = self.op(x)\n","\n","    if self.d_out > self.d_in:\n","      seq = np.array([[i for _ in range(self.d_out // self.d_in)] for i in range(self.d_in)]).flatten()\n","      return torch.cat(\n","          [out[:, i:i+1, :, :] for i in seq],\n","          dim = 1\n","          )\n","\n","    elif self.d_out < self.d_in:\n","        glen = self.d_in // self.d_out\n","        groups = torch.split(x, glen, dim=1)\n","        op1 = nn.AvgPool3d(kernel_size=(glen, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1)).cuda()\n","        out = []\n","        for g in groups:\n","            out.append(op1(g))\n","        out = torch.cat(out, dim=1)\n","        bn = nn.BatchNorm2d(self.d_out, affine = False).cuda()\n","        return bn(out)\n","\n","    return out"],"metadata":{"id":"Xn5dH7vB_8Qo","executionInfo":{"status":"ok","timestamp":1715413627772,"user_tz":420,"elapsed":2,"user":{"displayName":"Christian Granados","userId":"18308979293956140005"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["class ReLUConvBN(nn.Module):\n","\n","  def __init__(self, d_in, d_out, kernel_size, stride, padding, affine=True):\n","    super(ReLUConvBN, self).__init__()\n","    self.op = nn.Sequential(\n","      nn.ReLU(inplace = False),\n","      nn.Conv2d(d_in, d_out, kernel_size, stride = stride, padding = padding, bias = False),\n","      nn.BatchNorm2d(d_out, affine = affine)\n","    )\n","\n","  def forward(self, x):\n","    return self.op(x)"],"metadata":{"id":"Tf9qBnqK3LKN","executionInfo":{"status":"ok","timestamp":1715413627902,"user_tz":420,"elapsed":132,"user":{"displayName":"Christian Granados","userId":"18308979293956140005"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["class DilConv(nn.Module):\n","\n","  def __init__(self, d_in, d_out, kernel_size, stride, padding, dilation, affine = True):\n","    super(DilConv, self).__init__()\n","    self.op = nn.Sequential(\n","      nn.ReLU(inplace=False),\n","      nn.Conv2d(d_in, d_out, kernel_size = kernel_size, stride = stride, padding = padding, dilation = dilation, bias = False),\n","      nn.BatchNorm2d(d_out, affine = affine),\n","      )\n","\n","  def forward(self, x):\n","    return self.op(x)"],"metadata":{"id":"SsS-pbcs3Ifa","executionInfo":{"status":"ok","timestamp":1715413627902,"user_tz":420,"elapsed":5,"user":{"displayName":"Christian Granados","userId":"18308979293956140005"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["class DepthSepConv(nn.Module):\n","\n","  def __init__(self, d_in, d_out, kernel_size, stride, padding, affine = True):\n","    super(DepthSepConv, self).__init__()\n","    self.op = nn.Sequential(\n","      nn.ReLU(inplace = False),\n","      nn.Conv2d(d_in, d_in, kernel_size = kernel_size, stride = stride, padding = padding, groups = d_in, bias = False),\n","      nn.Conv2d(d_in, d_out, kernel_size = 1, padding = 0, bias = False),\n","      nn.BatchNorm2d(d_out, affine = affine)\n","    )\n","\n","  def forward(self, x):\n","    return self.op(x)\n"],"metadata":{"id":"S4HU3TVa5Tgj","executionInfo":{"status":"ok","timestamp":1715413627902,"user_tz":420,"elapsed":4,"user":{"displayName":"Christian Granados","userId":"18308979293956140005"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["class SpatialSepConv(nn.Module):\n","\n","  def __init__(self, d_in, d_out, kernel_size, stride, padding, affine = True):\n","    super(SpatialSepConv, self).__init__()\n","    self.op = nn.Sequential(\n","      nn.ReLU(inplace=False),\n","      nn.Conv2d(d_in, d_in, (1, kernel_size), stride = (1, stride), padding = (0, padding), bias = False),\n","      nn.Conv2d(d_in, d_out, (kernel_size, 1), stride = (stride, 1), padding = (padding, 0), bias = False),\n","      nn.BatchNorm2d(d_out, affine = affine)\n","      )\n","\n","  def forward(self, x):\n","    return self.op(x)"],"metadata":{"id":"8a3PN1xG7UX1","executionInfo":{"status":"ok","timestamp":1715413627902,"user_tz":420,"elapsed":4,"user":{"displayName":"Christian Granados","userId":"18308979293956140005"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["OPS = {\n","  'none' : lambda d_in, d_out, stride, affine: Zero(d_out, stride),\n","  'skip_connect' : lambda d_in, d_out, stride, affine: SkipConnection(d_in, d_out),\n","  'max_pool_3x3' : lambda d_in, d_out, stride, affine: MaxPoolBN(d_in, d_out, stride = stride),\n","  'avg_pool_3x3' : lambda d_in, d_out, stride, affine: AvgPoolBN(d_in, d_out, stride = stride),\n","  'dil_conv_3x3' : lambda d_in, d_out, stride, affine: DilConv(d_in, d_out, 3, stride, 2, 2, affine = affine),\n","  'dil_conv_5x5' : lambda d_in, d_out, stride, affine: DilConv(d_in, d_out, 5, stride, 4, 2, affine = affine),\n","  'depth_sep_conv_3x3' : lambda d_in, d_out, stride, affine: DepthSepConv(d_in, d_out, 3, stride, 1, affine = affine),\n","  'depth_sep_conv_5x5' : lambda d_in, d_out, stride, affine: DepthSepConv(d_in, d_out, 5, stride, 2, affine = affine),\n","  'spatial_sep_conv_5x5' : lambda d_in, d_out, stride, affine: SpatialSepConv(d_in, d_out, 5, stride, 2, affine = affine),\n","  'spatial_sep_conv_7x7' : lambda d_in, d_out, stride, affine: SpatialSepConv(d_in, d_out, 7, stride, 3, affine = affine)\n","}\n","\n","PRIMITIVES = [\n","    'none',\n","    'skip_connect',\n","    'max_pool_3x3',\n","    'avg_pool_3x3',\n","    'dil_conv_3x3',\n","    'dil_conv_5x5',\n","    'depth_sep_conv_3x3',\n","    'depth_sep_conv_5x5',\n","    'spatial_sep_conv_5x5',\n","    'spatial_sep_conv_7x7'\n","]"],"metadata":{"id":"-aFAAf-5DvMt","executionInfo":{"status":"ok","timestamp":1715413627902,"user_tz":420,"elapsed":4,"user":{"displayName":"Christian Granados","userId":"18308979293956140005"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["# 3. Mixed Operation"],"metadata":{"id":"n5bn4_NnFIFI"}},{"cell_type":"code","source":["class MixedOp(nn.Module):\n","\n","  def __init__(self, d_in, d_out, stride):\n","    super(MixedOp, self).__init__()\n","    self._ops = nn.ModuleList()\n","    for primitive in PRIMITIVES:\n","      op = OPS[primitive](d_in, d_out, stride, False)\n","      self._ops.append(op)\n","\n","  def forward(self, x, weights):\n","    return sum(w * op(x) for w, op in zip(weights, self._ops))"],"metadata":{"id":"FJ-hXbFeFHm3","executionInfo":{"status":"ok","timestamp":1715413627902,"user_tz":420,"elapsed":4,"user":{"displayName":"Christian Granados","userId":"18308979293956140005"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":["# 4. Cell"],"metadata":{"id":"8Nb94pgmFYCb"}},{"cell_type":"code","source":["class Cell(nn.Module):\n","\n","  def __init__(self, d_in, d_out, d_img_in, d_img_out):\n","    super(Cell, self).__init__()\n","\n","    self.num_ops = len(PRIMITIVES)\n","\n","    # Dimensions of the input feature map and output feature map\n","    # Used in initial operations s0 to increase storage information as we compress\n","    self.d_in = d_in\n","    self.d_out = d_out\n","\n","    # Dimensions of the input image size and output image size\n","    # Used for down/up sampling in initial operations (out of s0)\n","    self.d_img_in = d_img_in\n","    self.d_img_out = d_img_out\n","\n","    # Downsample\n","    if d_img_in > d_img_out:\n","      self.c0 = DownSampl(d_img_in, d_img_out)\n","\n","    # Upsample\n","    elif self.d_img_out > self.d_img_in:\n","      self.c0 = UpSampl(d_img_in, d_img_out)\n","\n","    else:\n","      self.c0 = lambda x: x\n","\n","    self._ops = nn.ModuleList()\n","\n","    for i in range(3):\n","      for j in range(i + 1):\n","        if j == 0:\n","          op = MixedOp(self.d_in, self.d_out // 2, 1)\n","        else:\n","          op = MixedOp(self.d_out // 2, self.d_out // 2, 1)\n","        self._ops.append(op)\n","\n","  def forward(self, s0, weights):\n","    # nops = num_ops = 10\n","    # Node 0 (s0) -> Node 1: weights (0 -> nops)\n","    # Node 0 (s0) -> Node 2: weights (nops -> 2 * nops)\n","    # Node 1 -> Node 2: weights (2 * nops -> 3 * nops)\n","    # Node 0 (s0) -> Node 3: weights (3 * nops -> 4 * nops)\n","    # Node 1 -> Node 3: weights (4 * nops -> 5 * nops)\n","    # Node 2 -> Node 3: weights (5 * nops -> 6 * nops)\n","    # Node 0, Node 1, Node 2, Node 3 -> SSB: weights (6 * nops -> 6 * nops + 4)\n","    # Node 0, Node 1, Node 2, Node 3 -> CSB: weights (6 * nops + 5 -> 6 * nops + 8)\n","    # Total Alphas per Cell: 68\n","\n","    s0 = self.c0(s0.cuda())\n","    concat_states = [s0]\n","    offset = 0\n","\n","    ssb_sum_weights = F.softmax(weights[-8:-4], dim = -1).cuda()\n","    csb_concat_weights = F.sigmoid(weights[-4:]).cuda()\n","\n","    for i in range(3):\n","\n","      s = sum(torch.stack([self._ops[offset + j](h, weights[(offset + j) * self.num_ops: (offset + j + 1) * self.num_ops]) for j, h in enumerate(concat_states)]))\n","      offset += len(concat_states)\n","      concat_states.append(s)\n","\n","    # Adding or removing feature maps from s0 to they can be combined w/ the output\n","    if (self.d_out // 2) > self.d_in:\n","      seq = np.array([[i for _ in range((self.d_out // 2) // self.d_in)] for i in range(self.d_in)]).flatten()\n","      s0 = torch.cat([s0[:, i:i+1, :, :] for i in seq], dim = 1)\n","\n","    elif (self.d_out // 2) < self.d_in:\n","      op0 = nn.Conv2d(self.d_in, self.d_out // 2, 1, groups = self.d_out // 2).cuda()\n","      s0 = op0(s0)\n","\n","    # Update s0 after they have been used as input to other branches to be able to be combined with output SSB and CSB\n","    concat_states[0] = s0\n","    shape = concat_states[0].shape\n","\n","    sums = sum([\n","        (torch.ones(shape).cuda() * ssb_sum_weights[i]) * concat_states[i] for i in range(len(concat_states))\n","        ])\n","\n","    csb_pre_out = torch.cat([\n","        (torch.ones(concat_states[i].shape).cuda() * csb_concat_weights[i]) * concat_states[i] for i in range(len(concat_states)\n","        )], dim = 1)\n","\n","    csb_out = nn.Conv2d(in_channels = (self.d_out // 2) * len(concat_states), out_channels = self.d_out // 2, kernel_size = 1, stride = 1, padding = 0, bias = False).cuda()(csb_pre_out)\n","\n","    return torch.cat([sums, csb_out], dim = 1)"],"metadata":{"id":"c_Kaljd1FXsc","executionInfo":{"status":"ok","timestamp":1715413627902,"user_tz":420,"elapsed":4,"user":{"displayName":"Christian Granados","userId":"18308979293956140005"}}},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":["# 5. Encoder/Decoder Backbone Cell"],"metadata":{"id":"B2efLvBJH4iP"}},{"cell_type":"code","source":["class BackBoneCell(nn.Module):\n","\n","  def __init__(self, d_in, d_out, d_img_in, d_img_out):\n","    '''\n","    Only the skip connections are tuned, the encoder and\n","    decoder cells remain the same. Implementation is the same\n","    as the original U-Net paper.\n","    '''\n","    super(BackBoneCell, self).__init__()\n","\n","    # Downsample\n","    if d_img_in > d_img_out:\n","        self.scale = DownSampl(d_img_in, d_img_out)\n","\n","    # Upsample\n","    elif d_img_out > d_img_in:\n","        self.scale = UpSampl(d_img_in, d_img_out)\n","\n","    else:\n","        self.scale = lambda x: x\n","\n","    self.op = nn.Sequential(\n","        nn.Conv2d(d_in, d_out, 3, padding = 1, stride = 1),\n","        nn.BatchNorm2d(d_out),\n","        nn.ReLU(inplace = False),\n","        nn.Conv2d(d_out, d_out, 3, padding = 1, stride = 1),\n","        nn.BatchNorm2d(d_out),\n","        nn.ReLU(inplace = False)\n","    )\n","\n","  def forward(self, x):\n","    out = self.scale(x)\n","    return self.op(out)"],"metadata":{"id":"KpPhZ574H4L-","executionInfo":{"status":"ok","timestamp":1715413627902,"user_tz":420,"elapsed":4,"user":{"displayName":"Christian Granados","userId":"18308979293956140005"}}},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":["# 6. Network"],"metadata":{"id":"hTITvBFSI980"}},{"cell_type":"code","source":["class Network(nn.Module):\n","\n","  def __init__(self):\n","    super(Network, self).__init__()\n","    self.num_ops = len(PRIMITIVES)\n","\n","    self.cells = nn.ModuleList()\n","\n","    # U-Net will be composed of a 5-step encoder, and 4-step decoder\n","    # Encoders gradually compress image but grow in channels\n","    # Decoder layers are all 80 feature maps thick, and are composed of a concatenation of 16 feature maps from all encoder nodes\n","    # We output two feature maps that are compared to the foreground and background grouth truth labels; compressed via 1x1 conv block\n","    # OPTION: If needed, can replace upsample with upconv instead of bilinear upscale\n","\n","    # Encoder 1: IMG 128x128 -> 128x128, FEAT 3 -> 8\n","    self.cells.append(BackBoneCell(3, 8, 128, 128))\n","\n","    # Encoder 2: IMG 128x128 -> 64x64, FEAT 8 -> 16\n","    self.cells.append(BackBoneCell(8, 16, 128, 64))\n","\n","    # Encoder 3: IMG 64x64 -> 32x32, FEAT 16 -> 32\n","    self.cells.append(BackBoneCell(16, 32, 64, 32))\n","\n","    # Encoder 4: IMG 32x32 -> 16x16, FEAT 32 -> 64\n","    self.cells.append(BackBoneCell(32, 64, 32, 16))\n","\n","    # Encoder 5: IMG 16x16 -> 8x8, FEAT 64 -> 128\n","    self.cells.append(BackBoneCell(64, 128, 16, 8))\n","\n","    # ---\n","\n","    # Skip_1_4: IMG 128x128 -> 16x16, FEAT 8 -> 32 (16 + 16)\n","    self.cells.append(Cell(8, 32, 128, 16))\n","\n","    # Skip_2_4: IMG 64x64 -> 16x16, FEAT 16 -> 32 (16 + 16)\n","    self.cells.append(Cell(16, 32, 64, 16))\n","\n","    # Skip_3_4: IMG 32x32 -> 16x16, FEAT 32 -> 32 (16 + 16)\n","    self.cells.append(Cell(32, 32, 32, 16))\n","\n","    # Skip_4_4: IMG 16x16 -> 16x16, FEAT 64 -> 32 (16 + 16)\n","    self.cells.append(Cell(64, 32, 16, 16))\n","\n","    # Enc_Dec_5_4: IMG 8x8 -> 16x16, FEAT 128 -> 16\n","    self.cells.append(BackBoneCell(128, 16, 8, 16))\n","\n","    # Decoder 4: IMG 16x16, FEAT 144 -> 16\n","    self.cells.append(BackBoneCell(144, 16, 16, 16))\n","\n","    # ---\n","\n","    # Skip_1_3: IMG 128x128 -> 32x32, FEAT 8 -> 32 (16 + 16)\n","    self.cells.append(Cell(8, 32, 128, 32))\n","\n","    # Skip_2_3: IMG 64x64 -> 32x32, FEAT 16 -> 32 (16 + 16)\n","    self.cells.append(Cell(16, 32, 64, 32))\n","\n","    # Skip_3_3: IMG 32x32 -> 32x32, FEAT 32 -> 32 (16 + 16)\n","    self.cells.append(Cell(32, 32, 32, 32))\n","\n","    # Skip_4_3: IMG 16x16 -> 32x32, FEAT 64 -> 32 (16 + 16)\n","    self.cells.append(Cell(64, 32, 16, 32))\n","\n","    # Dec_4_3: IMG 16x16 -> 32x32, FEAT 16 -> 16\n","    self.cells.append(BackBoneCell(16, 16, 16, 32))\n","\n","    # Decoder 3: IMG 32x32, FEAT 144 -> 16\n","    self.cells.append(BackBoneCell(144, 16, 32, 32))\n","\n","    # ---\n","\n","    # Skip_1_2: IMG 128x128 -> 64x64, FEAT 8 -> 32 (16 + 16)\n","    self.cells.append(Cell(8, 32, 128, 64))\n","\n","    # Skip_2_2: IMG 64x64 -> 64x64, FEAT 16 -> 32 (16 + 16)\n","    self.cells.append(Cell(16, 32, 64, 64))\n","\n","    # Skip_3_2: IMG 32x32 -> 64x64, FEAT 32 -> 32 (16 + 16)\n","    self.cells.append(Cell(32, 32, 32, 64))\n","\n","    # Skip_4_2: IMG 16x16 -> 64x64, FEAT 64 -> 32 (16 + 16)\n","    self.cells.append(Cell(64, 32, 16, 64))\n","\n","    # Dec_3_2: IMG 32x32 -> 64x64, FEAT 16 -> 16\n","    self.cells.append(BackBoneCell(16, 16, 32, 64))\n","\n","    # Decoder 2: IMG 64x64, FEAT 144 -> 16\n","    self.cells.append(BackBoneCell(144, 16, 64, 64))\n","\n","    # ---\n","\n","    # Skip_1_1: IMG 128x128 -> 128x128, FEAT 8 -> 32 (16 + 16)\n","    self.cells.append(Cell(8, 32, 128, 128))\n","\n","    # Skip_2_1: IMG 64x64 -> 128x128, FEAT 16 -> 32 (16 + 16)\n","    self.cells.append(Cell(16, 32, 64, 128))\n","\n","    # Skip_3_1: IMG 32x32 -> 128x128, FEAT 32 -> 32 (16 + 16)\n","    self.cells.append(Cell(32, 32, 32, 128))\n","\n","    # Skip_4_1: IMG 16x16 -> 128x128, FEAT 64 -> 32 (16 + 16)\n","    self.cells.append(Cell(64, 32, 16, 128))\n","\n","    # Enc_Dec_2_1: IMG 64x64 -> 128x128, FEAT 16 -> 16\n","    self.cells.append(BackBoneCell(16, 16, 64, 128))\n","\n","    # Dec_Out_1 1: IMG 128x128, FEAT 144 -> 2\n","    self.cells.append(BackBoneCell(144, 2, 128, 128))\n","\n","    self.initialize_alphas()\n","\n","  def initialize_alphas(self):\n","    self._arch_parameters = Variable(1e-3 * torch.randn(16, 6 * self.num_ops + 8).cuda(), requires_grad = True)\n","\n","  def logmeanexp(self, x):\n","    x_max, _ = torch.max(x, dim = -1, keepdim=True)\n","    return x_max + torch.log(torch.mean(torch.exp(x - x_max), dim = -1, keepdim = True))\n","\n","  def mlc_loss(self):\n","    n = self.num_ops\n","    return torch.cat((\n","      self.logmeanexp(self._arch_parameters[:, : n]).flatten(), # Node 1\n","      self.logmeanexp(self._arch_parameters[:, n : 3 * n]).flatten(), # Node 2\n","      self.logmeanexp(self._arch_parameters[:, 3 * n : 6 * n]).flatten() # Node 3\n","    ), dim = 0)\n","\n","  def arch_parameters(self):\n","    return self._arch_parameters\n","\n","  def forward(self, inp):\n","\n","    # Encoder Branch\n","    enc1 = self.cells[0](inp)\n","    enc2 = self.cells[1](enc1)\n","    enc3 = self.cells[2](enc2)\n","    enc4 = self.cells[3](enc3)\n","    enc5 = self.cells[4](enc4)\n","\n","    # Decoder 4\n","    skip_1_4 = self.cells[5](enc1, self._arch_parameters[0, :])\n","    skip_2_4 = self.cells[6](enc2, self._arch_parameters[1, :])\n","    skip_3_4 = self.cells[7](enc3, self._arch_parameters[2, :])\n","    skip_4_4 = self.cells[8](enc4, self._arch_parameters[3, :])\n","\n","    enc_dec_5_4 = self.cells[9](enc5)\n","\n","    dec_4 = self.cells[10](\n","        torch.cat((\n","            skip_1_4,\n","            skip_2_4,\n","            skip_3_4,\n","            skip_4_4,\n","            enc_dec_5_4\n","            ), dim = 1)\n","        )\n","\n","    # Decoder 3\n","    skip_1_3 = self.cells[11](enc1, self._arch_parameters[4, :])\n","    skip_2_3 = self.cells[12](enc2, self._arch_parameters[5, :])\n","    skip_3_3 = self.cells[13](enc3, self._arch_parameters[6, :])\n","    skip_4_3 = self.cells[14](enc4, self._arch_parameters[7, :])\n","\n","    dec_4_3 = self.cells[15](dec_4)\n","\n","    dec_3 = self.cells[16](\n","        torch.cat((\n","            skip_1_3,\n","            skip_2_3,\n","            skip_3_3,\n","            skip_4_3,\n","            dec_4_3\n","            ), dim = 1)\n","        )\n","\n","    # Decoder 2\n","    skip_1_2 = self.cells[17](enc1, self._arch_parameters[8, :])\n","    skip_2_2 = self.cells[18](enc2, self._arch_parameters[9, :])\n","    skip_3_2 = self.cells[19](enc3, self._arch_parameters[10, :])\n","    skip_4_2 = self.cells[20](enc4, self._arch_parameters[11, :])\n","\n","    dec_3_2 = self.cells[21](dec_3)\n","\n","    dec_2 = self.cells[22](\n","        torch.cat((\n","            skip_1_2,\n","            skip_2_2,\n","            skip_3_2,\n","            skip_4_2,\n","            dec_3_2\n","            ), dim = 1)\n","        )\n","\n","    # Decoder 1\n","    skip_1_1 = self.cells[23](enc1, self._arch_parameters[12, :])\n","    skip_2_1= self.cells[24](enc2, self._arch_parameters[13, :])\n","    skip_3_1 = self.cells[25](enc3, self._arch_parameters[14, :])\n","    skip_4_1 = self.cells[26](enc4, self._arch_parameters[15, :])\n","\n","    dec_2_1 = self.cells[27](dec_2)\n","\n","    dec_1 = self.cells[28](\n","        torch.cat((\n","            skip_1_1,\n","            skip_2_1,\n","            skip_3_1,\n","            skip_4_1,\n","            dec_2_1\n","        ), dim = 1))\n","\n","    return dec_1"],"metadata":{"id":"RyzazrBDI9jk","executionInfo":{"status":"ok","timestamp":1715413627902,"user_tz":420,"elapsed":4,"user":{"displayName":"Christian Granados","userId":"18308979293956140005"}}},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":["# 7. Training Utils"],"metadata":{"id":"tWnIwHDFobsX"}},{"cell_type":"code","source":["# Stores Accuracy, IoU, and Dice data\n","class AvgrageMeter(object):\n","\n","    def __init__(self):\n","        super().__init__()\n","        self.avg = 0\n","        self.sum = 0\n","        self.cnt = 0\n","        self.history = np.array([])\n","\n","    def update(self, val, n = 1):\n","        self.sum += val * n\n","        self.cnt += n\n","        self.avg = self.sum / self.cnt\n","        self.history = np.append(self.history, self.avg)\n","\n","    def getAverage(self):\n","        return self.avg\n","\n","    def getHistory(self):\n","        return self.history"],"metadata":{"id":"EtHknnXqo-sh","executionInfo":{"status":"ok","timestamp":1715413627902,"user_tz":420,"elapsed":3,"user":{"displayName":"Christian Granados","userId":"18308979293956140005"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["# Accuracy calculation\n","def accuracy(inp, tar):\n","    acc, inp, tar = [], inp.int(), tar.int()\n","    for i in range(batch_size):\n","        for j in range(2):\n","            cinp, ctar = inp[i, j, :, :], tar[i, j, :, :]\n","            acc.append(torch.sum(cinp == ctar).item() / ctar.numel())\n","    return np.mean(acc)\n","\n","# IoU Calculation\n","def iou(inp, tar):\n","    ciou, inp, tar = [], inp.int(), tar.int()\n","    for i in range(batch_size):\n","        for j in range(2):\n","            cinp, ctar = inp[i, j, :, :], tar[i, j, :, :]\n","            intersection = torch.sum(torch.logical_and(cinp, ctar))\n","            union = torch.sum(torch.logical_or(cinp, ctar))\n","            ciou.append(intersection.item() / union.item())\n","    return np.mean(ciou)\n","\n","\n","# Dice Calculation\n","def dice(inp, tar):\n","    cdice, inp, tar = [], inp.int(), tar.int()\n","    for i in range(batch_size):\n","        for j in range(2):\n","            cinp, ctar = inp[i, j, :, :], tar[i, j, :, :]\n","            intersection = torch.sum(torch.logical_and(cinp, ctar))\n","            total_area = torch.sum(cinp) + torch.sum(ctar)\n","            cdice.append(2 * intersection.item() / total_area.item())\n","    return np.mean(cdice)\n"],"metadata":{"id":"cjHmNK48pYgN","executionInfo":{"status":"ok","timestamp":1715413627902,"user_tz":420,"elapsed":3,"user":{"displayName":"Christian Granados","userId":"18308979293956140005"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["# Infer - Assessing Network Preformance\n","def infer(valid_queue, model, criterion, discrete = False):\n","    model.eval()\n","\n","    test_acc = AvgrageMeter()\n","    test_iou = AvgrageMeter()\n","    test_dice = AvgrageMeter()\n","\n","    for step, (inputd, target) in enumerate(valid_queue):\n","\n","        inputd = inputd.cuda()\n","        target = target.cuda(non_blocking=True)\n","\n","        logits = model(inputd)\n","        loss = criterion(logits, target)\n","        logits_binary = (logits >= 0.5).int()\n","\n","        if discrete:\n","            test_acc.update(accuracy(logits_binary[:, :1, :, :], target[:, :1, :, :]), n = batch_size)\n","            test_iou.update(iou(logits_binary[:, :1, :, :], target[:, :1, :, :]), n = batch_size)\n","            test_dice.update(dice(logits_binary[:, :1, :, :], target[:, :1, :, :]), n = batch_size)\n","\n","        else:\n","            test_acc.update(accuracy(logits_binary, target), n = batch_size)\n","            test_iou.update(iou(logits_binary, target), n = batch_size)\n","            test_dice.update(dice(logits_binary, target), n = batch_size)\n","\n","    return test_acc.getAverage(), test_iou.getAverage(), test_dice.getAverage()"],"metadata":{"id":"cUooNBUDcmYq","executionInfo":{"status":"ok","timestamp":1715413627902,"user_tz":420,"elapsed":3,"user":{"displayName":"Christian Granados","userId":"18308979293956140005"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["# Training (Inner-Epoch)\n","def train(train_queue, valid_queue, model, a_optimizer, criterion, w_optimizer, lr, scheduler, epoch, beta_weight):\n","\n","    objs_ = AvgrageMeter()\n","    acc_ = AvgrageMeter()\n","    iou_ = AvgrageMeter()\n","    dice_ = AvgrageMeter()\n","\n","    weights = 0 + 50 * epoch / 100\n","\n","    for step, (inp, tar) in enumerate(train_queue):\n","\n","        model.train()\n","        n = inp.size(0)\n","\n","        inp = inp.cuda()\n","        tar = tar.cuda(non_blocking=True)\n","\n","        inp_search, tar_search = next(iter(valid_queue))\n","        inp_search = inp_search.cuda()\n","        tar_search = tar_search.cuda(non_blocking=True)\n","\n","        a_optimizer.zero_grad()\n","        logits = model(inp_search)\n","        loss = criterion(logits, tar_search) + weights * model.mlc_loss()\n","        loss.mean().backward()\n","        a_optimizer.step()\n","\n","        w_optimizer.zero_grad()\n","        logits = model(inp)\n","        loss = criterion(logits, tar)\n","        loss.mean().backward()\n","        nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n","        w_optimizer.step()\n","\n","        objs_.update(loss.item())\n","        acc_.update(accuracy(logits, tar), n = batch_size)\n","        iou_.update(iou(logits, tar), n = batch_size)\n","        dice_.update(dice(logits, tar), n = batch_size)\n","\n","        if step % rep_freq == 0:\n","            print(\"Training Step:\", step,\n","                  \"Loss:\", objs_.getAverage(),\n","                  \"Accuracy\", acc_.getAverage(),\n","                  \"IoU\", iou_.getAverage(),\n","                  \"Dice\", dice_.getAverage()\n","                  )\n","            row_list.append({\n","                \"Time\": datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\"),\n","                \"Epoch\": epoch,\n","                \"Step\": step,\n","                \"LR\": lr,\n","                \"Loss\": objs_.getAverage(),\n","                \"Accuracy\": acc_.getAverage(),\n","                \"IoU\": iou_.getAverage(),\n","                \"Dice\": dice_.getAverage(),\n","                \"Node_1_A\": [],\n","                \"Node_2_A\": [],\n","                \"Node_3_A\": [],\n","                \"SSB_A\": [],\n","                \"CSB_A\": []\n","            })\n"],"metadata":{"id":"w5HdD45tn7ts","executionInfo":{"status":"ok","timestamp":1715413628015,"user_tz":420,"elapsed":116,"user":{"displayName":"Christian Granados","userId":"18308979293956140005"}}},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":["# 8. Training\n","**NOTE:** The modified [search space paper](https://openreview.net/forum?id=2IkLprQjby) utilizes SGD for the filter weights and ADAM to optimize alpha values. No peudo-hessian, as included in the original DARTs paper, is used for updating weights. Likely faster, but may be less accurate.\n","\n","**NOTE:** There are two output maps, one for the target class and one for the background class. Metrics like accuracy, IoU, and Dice are calculated on both of these feature maps. This is done for a more wholistic view of the network preformance. However, during inference, only the target class output will be used. We have two seperate output classes during training to allow the network to learn target and background classes independently."],"metadata":{"id":"vZ07-dK4-ozn"}},{"cell_type":"code","source":["# Hyperparameters\n","num_ops = len(PRIMITIVES)\n","rep_freq = 8\n","batch_size = 24\n","epochs = 30\n","lr = 0.025\n","lr_min = 0.0003\n","momentum = 0.9\n","arch_lr = 0.0003\n","beta_weight = 0.5\n","weight_decay = 0.0003\n","drop_path = 0.1\n","grad_clip = 5.0\n","data_len = 2048\n","seed = 2024\n","\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)"],"metadata":{"id":"q9Jvn24XCSJY","executionInfo":{"status":"ok","timestamp":1715413628015,"user_tz":420,"elapsed":3,"user":{"displayName":"Christian Granados","userId":"18308979293956140005"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["# Load Data\n","train_data = HAM10000_Dataset(\"/content/drive/MyDrive/Granados_Thesis_SP24/HAM10000/image_indexing.csv\", num_entries = data_len)\n","\n","# 70% Train/ 20% Validation/ 10% Test\n","indices = list(range(data_len))\n","train_val_split = int(np.floor(0.7 * len(train_data)))\n","val_test_split = int(np.floor(0.9 * len(train_data)))\n","\n","train_queue = torch.utils.data.DataLoader(\n","    train_data,\n","    batch_size = batch_size,\n","    sampler = torch.utils.data.sampler.SubsetRandomSampler(indices[:train_val_split]),\n","    pin_memory = True,\n","    drop_last = True\n","    )\n","\n","val_queue = torch.utils.data.DataLoader(\n","    train_data,\n","    batch_size = batch_size,\n","    sampler = torch.utils.data.sampler.SubsetRandomSampler(indices[train_val_split:val_test_split]),\n","    pin_memory = True,\n","    drop_last = True\n","    )\n","\n","test_queue = torch.utils.data.DataLoader(\n","    train_data,\n","    batch_size = batch_size,\n","    sampler = torch.utils.data.sampler.SubsetRandomSampler(indices[val_test_split:]),\n","    pin_memory = True,\n","    drop_last = True\n","    )"],"metadata":{"id":"2qLgMKvIa_4l","executionInfo":{"status":"ok","timestamp":1715413628015,"user_tz":420,"elapsed":3,"user":{"displayName":"Christian Granados","userId":"18308979293956140005"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["# Logging Dataframe\n","columns_ = [\n","    \"Time\", \"Epoch\", \"Step\", \"LR\",\n","    \"Loss\", \"Accuracy\", \"IoU\", \"Dice\",\n","    \"Node_1_A\", \"Node_2_A\", \"Node_3_A\", \"SSB_A\", \"CSB_A\"]\n","row_list = []"],"metadata":{"id":"qYkURr5UnQGW","executionInfo":{"status":"ok","timestamp":1715413628015,"user_tz":420,"elapsed":2,"user":{"displayName":"Christian Granados","userId":"18308979293956140005"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["torch.cuda.empty_cache()\n","if not torch.cuda.is_available():\n","  print(\"No GPU Device Availible\")\n","\n","# Model\n","model = Network().cuda()\n","\n","# Number of Parameters\n","w_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","a_params = sum(p.numel() for p in model.arch_parameters())\n","print(\"Total Parameters:\", w_params + a_params)\n","\n","# Loss\n","criterion = nn.BCEWithLogitsLoss().cuda()\n","\n","# Optimizers\n","w_optimizer = torch.optim.SGD(model.parameters(), lr = lr, momentum = momentum, weight_decay = weight_decay)\n","a_optimizer = torch.optim.RAdam([model.arch_parameters()], lr = arch_lr, betas = (0.5, 0.999))\n","\n","# Scheduler\n","scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(w_optimizer, float(epochs), eta_min = lr_min)\n","\n","# Train (Inter-Epoch)\n","for epoch in range(epochs):\n","    lr = scheduler.get_lr()[0]\n","    print(f\"epoch {epoch} lr {lr}\")\n","\n","    train(train_queue, val_queue, model, a_optimizer, criterion, w_optimizer, lr, scheduler, epoch, beta_weight)\n","\n","    with torch.no_grad():\n","\n","        alpha_tensor = model.arch_parameters()\n","\n","        for i, a in enumerate(alpha_tensor):\n","\n","            a = a.cuda().data\n","\n","            node_1_alphas = [np.round(r.item(), 4) for r in a[: num_ops]]\n","            node_2_alphas = [np.round(r.item(), 4) for r in a[num_ops : 3 * num_ops]]\n","            node_3_alphas = [np.round(r.item(), 4) for r in a[3 * num_ops : 6 * num_ops]]\n","            ssb_alphas = [np.round(r.item(), 4) for r in a[-8:-4]]\n","            csb_alphas = [np.round(r.item(), 4) for r in a[-4:]]\n","\n","            row_list.append({\n","                \"Time\": datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\"),\n","                \"Epoch\": epoch,\n","                \"Step\": -1,\n","                \"LR\": lr,\n","                \"Loss\": -1,\n","                \"Accuracy\": -1,\n","                \"IoU\": -1,\n","                \"Dice\": -1,\n","                \"Node_1_A\": node_1_alphas,\n","                \"Node_2_A\": node_2_alphas,\n","                \"Node_3_A\": node_3_alphas,\n","                \"SSB_A\": ssb_alphas,\n","                \"CSB_A\": csb_alphas\n","            })\n","\n","            print(\n","                \"Node_1_Alphas\", node_1_alphas,\n","                \"Node_2_Alphas\", node_2_alphas,\n","                \"Node_3_Alphas\", node_3_alphas,\n","                \"SSB_Alphas\", ssb_alphas,\n","                \"CSB_Alphas\", csb_alphas,\n","                )\n","\n","        if epoch == epochs - 1:\n","            test_acc, test_iou, test_dice = infer(test_queue, model, criterion)\n","            print(\"Test Accuracy:\", test_acc, \"Test IoU:\", test_iou, \"Test Dice:\", test_dice)\n","            row_list.append({\n","                \"Time\": datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\"),\n","                \"Epoch\": epoch,\n","                \"Step\": -1,\n","                \"LR\": lr,\n","                \"Loss\": -1,\n","                \"Accuracy\": test_acc,\n","                \"IoU\": test_iou,\n","                \"Dice\": test_dice,\n","                \"Node_1_A\": [],\n","                \"Node_2_A\": [],\n","                \"Node_3_A\": [],\n","                \"SSB_A\": [],\n","                \"CSB_A\": []\n","            })\n","\n","    scheduler.step()\n","    ctime = datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")\n","    torch.save(model.state_dict(), f\"/content/drive/MyDrive/Granados_Thesis_SP24/Models/{ctime}_epoch_{epoch}.pt\")"],"metadata":{"id":"56KPJYHvZnHH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Assemble & Save log\n","log_df = pd.DataFrame(row_list, columns = columns_)\n","log_df.to_csv('/content/drive/MyDrive/Granados_Thesis_SP24/log_df.csv')"],"metadata":{"id":"sAa2ARTN9YdY","executionInfo":{"status":"ok","timestamp":1715325042415,"user_tz":420,"elapsed":227,"user":{"displayName":"Christian Granados","userId":"18308979293956140005"}}},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":["# 9. Testing\n","**Note:** During discretization, both SSB and CSB blocks are mean-gated. Meaning we only keep connections that are above the mean alpha values of all the connections feeding into the block.   \n","\n","### 9.1 Discrete Weight Parsing"],"metadata":{"id":"FB87AHLkYX-N"}},{"cell_type":"code","source":["log_df = pd.read_csv(\"/content/drive/MyDrive/Granados_Thesis_SP24/log_df.csv\").drop(columns = [\"Unnamed: 0\"])"],"metadata":{"id":"zVP25She1nW3","executionInfo":{"status":"ok","timestamp":1715413628015,"user_tz":420,"elapsed":2,"user":{"displayName":"Christian Granados","userId":"18308979293956140005"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["top_n_ops = 3\n","finalnet_node_tuples, finalnet_csb_tuples, finalnet_ssb_tuples = [], [], []\n","finalnet_df = log_df[-17:-1][[\"Node_1_A\", \"Node_2_A\", \"Node_3_A\", \"SSB_A\", \"CSB_A\"]].applymap(lambda x: ast.literal_eval(x))\n","\n","for row in finalnet_df.iterrows():\n","  ctuples = []\n","  for i, w in enumerate(row[1][:3]):\n","    nlar = [w.index(j) for j in heapq.nlargest(top_n_ops, w)]\n","    for n in nlar:\n","      inp_node = n // len(PRIMITIVES)\n","      oper = n % len(PRIMITIVES)\n","      ctuples.append([inp_node, i + 1, PRIMITIVES[oper]])\n","  finalnet_node_tuples.append(ctuples)\n","\n","  ssb_mean, csb_mean = np.mean(row[1][3]), np.mean(row[1][4])\n","  finalnet_ssb_tuples.append([i for i, w in enumerate(row[1][3]) if w >= ssb_mean])\n","  finalnet_csb_tuples.append([i for i, w in enumerate(row[1][4]) if w >= csb_mean])"],"metadata":{"id":"qJvjKJS-h2RX","executionInfo":{"status":"ok","timestamp":1715416404460,"user_tz":420,"elapsed":109,"user":{"displayName":"Christian Granados","userId":"18308979293956140005"}}},"execution_count":72,"outputs":[]},{"cell_type":"markdown","source":["### 9.2 Discrete Cell"],"metadata":{"id":"3zxz90GHcuZE"}},{"cell_type":"code","source":["class DiscreteCell(nn.Module):\n","\n","  def __init__(self, d_in, d_out, d_img_in, d_img_out, node_tuples, csb_tuples, ssb_tuples):\n","    super(DiscreteCell, self).__init__()\n","    '''\n","    node_tuples in the form (input node, output node, operation) and detail the discrete\n","    operation choosen.\n","\n","    csb_tuples and ssb_tuples are a list of node indicies. They are used to choose\n","    which output nodes get drawn from.\n","    '''\n","    self.node_tuples = sorted(node_tuples, key = lambda x: x[1] if x[0] == 0 else int(str(x[0]) + str(x[1])))\n","    self.num_ops = len(PRIMITIVES)\n","    self.csb_tuples = csb_tuples\n","    self.ssb_tuples = ssb_tuples\n","\n","    # Set of all nodes that output to another node\n","    self.out_set = list(set(csb_tuples + ssb_tuples + list(list(zip(*node_tuples))[0])))\n","\n","    # Dimensions of the input feature map and output feature map\n","    # Used in initial operations s0 to increase storage information as we compress\n","    self.d_in = d_in\n","    self.d_out = d_out\n","\n","    # Dimensions of the input image size and output image size\n","    # Used for down/up sampling in initial operations (out of s0)\n","    self.d_img_in = d_img_in\n","    self.d_img_out = d_img_out\n","\n","    # Downsample\n","    if d_img_in > d_img_out:\n","      self.c0 = DownSampl(d_img_in, d_img_out)\n","\n","    # Upsample\n","    elif self.d_img_out > self.d_img_in:\n","      self.c0 = UpSampl(d_img_in, d_img_out)\n","\n","    else:\n","      self.c0 = lambda x: x\n","\n","    self.nodes = [None, None, None, None]\n","\n","    self.ops_dict = nn.ModuleDict()\n","\n","    for inp, out, opn in self.node_tuples:\n","        if inp == 0:\n","            self.ops_dict[str(inp) + str(out) + opn] = OPS[opn](self.d_in, self.d_out // 2, 1, False).cuda()\n","        else:\n","            self.ops_dict[str(inp) + str(out) + opn] = OPS[opn](self.d_out // 2, self.d_out // 2, 1, False).cuda()\n","\n","  def forward(self, s0):\n","\n","    s0 = self.c0(s0.cuda())\n","    self.nodes[0] = s0\n","\n","    for i in range(1, 4):\n","        inputs = filter(lambda x: x[1] == i, self.node_tuples)\n","        if i in self.out_set:\n","            tensor_list = []\n","            for inp, out, opn in inputs:\n","                tensor_list.append(self.ops_dict[str(inp) + str(out) + opn](self.nodes[inp]))\n","            self.nodes[i] = sum(tensor_list)\n","\n","    # Adding or removing feature maps from s0 to they can be combined w/ the output\n","    if (self.d_out // 2) > self.d_in:\n","        seq = np.array([[i for _ in range((self.d_out // 2) // self.d_in)] for i in range(self.d_in)]).flatten()\n","        s0 = torch.cat([s0[:, i:i+1, :, :] for i in seq], dim = 1)\n","\n","    elif (self.d_out // 2) < self.d_in:\n","        op0 = nn.Conv2d(self.d_in, self.d_out // 2, 1, groups = self.d_out // 2).cuda()\n","        s0 = op0(s0)\n","\n","    self.nodes[0] = s0\n","\n","    ssb = sum([self.nodes[i] for i in self.ssb_tuples])\n","    csb_pre = torch.cat([self.nodes[i] for i in self.csb_tuples], dim = 1)\n","    csb_out = nn.Conv2d(in_channels = (self.d_out // 2) * len(self.csb_tuples), out_channels = self.d_out // 2, kernel_size = 1, stride = 1, padding = 0, bias = False).cuda()(csb_pre)\n","\n","    return torch.cat([ssb, csb_out], dim = 1)"],"metadata":{"id":"NLqIDN9Uh1F3","executionInfo":{"status":"ok","timestamp":1715414284781,"user_tz":420,"elapsed":112,"user":{"displayName":"Christian Granados","userId":"18308979293956140005"}}},"execution_count":62,"outputs":[]},{"cell_type":"markdown","source":["### 9.3 Discrete Network"],"metadata":{"id":"6Dq73M3TcgXc"}},{"cell_type":"code","source":["class DiscreteNetwork(nn.Module):\n","\n","  def __init__(self, node_tuples, csb_tuples, ssb_tuples, n = 1):\n","    super(DiscreteNetwork, self).__init__()\n","    self.num_ops = len(PRIMITIVES)\n","\n","    self.cells = nn.ModuleList()\n","\n","    # U-Net will be composed of a 5-step encoder, and 4-step decoder\n","    # Encoders gradually compress image but grow in channels\n","    # Decoder layers are all 80 feature maps thick, and are composed of a concatenation of 16 feature maps from all encoder nodes\n","    # We output two feature maps that are compared to the foreground and background grouth truth labels; compressed via 1x1 conv block\n","    # OPTION: n controls how large the hidden layers are via a multiplicative factor\n","\n","    # Encoder 1: IMG 128x128 -> 128x128, FEAT 3 -> 8\n","    self.cells.append(BackBoneCell(3, 8 * n, 128, 128))\n","\n","    # Encoder 2: IMG 128x128 -> 64x64, FEAT 8 -> 16\n","    self.cells.append(BackBoneCell(8 * n, 16 * n, 128, 64))\n","\n","    # Encoder 3: IMG 64x64 -> 32x32, FEAT 16 -> 32\n","    self.cells.append(BackBoneCell(16 * n, 32 * n, 64, 32))\n","\n","    # Encoder 4: IMG 32x32 -> 16x16, FEAT 32 -> 64\n","    self.cells.append(BackBoneCell(32 * n, 64 * n, 32, 16))\n","\n","    # Encoder 5: IMG 16x16 -> 8x8, FEAT 64 -> 128\n","    self.cells.append(BackBoneCell(64 * n, 128 * n, 16, 8))\n","\n","    # ---\n","\n","    # Skip_1_4: IMG 128x128 -> 16x16, FEAT 8 -> 32 (16 + 16)\n","    self.cells.append(DiscreteCell(8 * n, 32 * n, 128, 16, node_tuples[0], csb_tuples[0], ssb_tuples[0]))\n","\n","    # Skip_2_4: IMG 64x64 -> 16x16, FEAT 16 -> 32 (16 + 16)\n","    self.cells.append(DiscreteCell(16 * n, 32 * n, 64, 16, node_tuples[1], csb_tuples[1], ssb_tuples[1]))\n","\n","    # Skip_3_4: IMG 32x32 -> 16x16, FEAT 32 -> 32 (16 + 16)\n","    self.cells.append(DiscreteCell(32 * n, 32 * n, 32, 16, node_tuples[2], csb_tuples[2], ssb_tuples[2]))\n","\n","    # Skip_4_4: IMG 16x16 -> 16x16, FEAT 64 -> 32 (16 + 16)\n","    self.cells.append(DiscreteCell(64 * n, 32 * n, 16, 16, node_tuples[3], csb_tuples[3], ssb_tuples[3]))\n","\n","    # Enc_Dec_5_4: IMG 8x8 -> 16x16, FEAT 128 -> 16\n","    self.cells.append(BackBoneCell(128 * n, 16 * n, 8, 16))\n","\n","    # Decoder 4: IMG 16x16, FEAT 144 -> 16\n","    self.cells.append(BackBoneCell(144 * n, 16 * n, 16, 16))\n","\n","    # ---\n","\n","    # Skip_1_3: IMG 128x128 -> 32x32, FEAT 8 -> 32 (16 + 16)\n","    self.cells.append(DiscreteCell(8 * n, 32 * n, 128, 32, node_tuples[4], csb_tuples[4], ssb_tuples[4]))\n","\n","    # Skip_2_3: IMG 64x64 -> 32x32, FEAT 16 -> 32 (16 + 16)\n","    self.cells.append(DiscreteCell(16 * n, 32 * n, 64, 32, node_tuples[5], csb_tuples[5], ssb_tuples[5]))\n","\n","    # Skip_3_3: IMG 32x32 -> 32x32, FEAT 32 -> 32 (16 + 16)\n","    self.cells.append(DiscreteCell(32 * n, 32 * n, 32, 32, node_tuples[6], csb_tuples[6], ssb_tuples[6]))\n","\n","    # Skip_4_3: IMG 16x16 -> 32x32, FEAT 64 -> 32 (16 + 16)\n","    self.cells.append(DiscreteCell(64 * n, 32 * n, 16, 32, node_tuples[7], csb_tuples[7], ssb_tuples[7]))\n","\n","    # Dec_4_3: IMG 16x16 -> 32x32, FEAT 16 -> 16\n","    self.cells.append(BackBoneCell(16 * n, 16 * n, 16, 32))\n","\n","    # Decoder 3: IMG 32x32, FEAT 144 -> 16\n","    self.cells.append(BackBoneCell(144 * n, 16 * n, 32, 32))\n","\n","    # ---\n","\n","    # Skip_1_2: IMG 128x128 -> 64x64, FEAT 8 -> 32 (16 + 16)\n","    self.cells.append(DiscreteCell(8 * n, 32 * n, 128, 64, node_tuples[8], csb_tuples[8], ssb_tuples[8]))\n","\n","    # Skip_2_2: IMG 64x64 -> 64x64, FEAT 16 -> 32 (16 + 16)\n","    self.cells.append(DiscreteCell(16 * n, 32 * n, 64, 64, node_tuples[9], csb_tuples[9], ssb_tuples[9]))\n","\n","    # Skip_3_2: IMG 32x32 -> 64x64, FEAT 32 -> 32 (16 + 16)\n","    self.cells.append(DiscreteCell(32 * n, 32 * n, 32, 64, node_tuples[10], csb_tuples[10], ssb_tuples[10]))\n","\n","    # Skip_4_2: IMG 16x16 -> 64x64, FEAT 64 -> 32 (16 + 16)\n","    self.cells.append(DiscreteCell(64 * n, 32 * n, 16, 64, node_tuples[11], csb_tuples[11], ssb_tuples[11]))\n","\n","    # Dec_3_2: IMG 32x32 -> 64x64, FEAT 16 -> 16\n","    self.cells.append(BackBoneCell(16 * n, 16 * n, 32, 64))\n","\n","    # Decoder 2: IMG 64x64, FEAT 144 -> 16\n","    self.cells.append(BackBoneCell(144 * n, 16 * n, 64, 64))\n","\n","    # ---\n","\n","    # Skip_1_1: IMG 128x128 -> 128x128, FEAT 8 -> 32 (16 + 16)\n","    self.cells.append(DiscreteCell(8 * n, 32 * n, 128, 128, node_tuples[12], csb_tuples[12], ssb_tuples[12]))\n","\n","    # Skip_2_1: IMG 64x64 -> 128x128, FEAT 16 -> 32 (16 + 16)\n","    self.cells.append(DiscreteCell(16 * n, 32 * n, 64, 128, node_tuples[13], csb_tuples[13], ssb_tuples[13]))\n","\n","    # Skip_3_1: IMG 32x32 -> 128x128, FEAT 32 -> 32 (16 + 16)\n","    self.cells.append(DiscreteCell(32 * n, 32 * n, 32, 128, node_tuples[14], csb_tuples[14], ssb_tuples[14]))\n","\n","    # Skip_4_1: IMG 16x16 -> 128x128, FEAT 64 -> 32 (16 + 16)\n","    self.cells.append(DiscreteCell(64 * n, 32 * n, 16, 128, node_tuples[15], csb_tuples[15], ssb_tuples[15]))\n","\n","    # Enc_Dec_2_1: IMG 64x64 -> 128x128, FEAT 16 -> 16\n","    self.cells.append(BackBoneCell(16 * n, 16 * n, 64, 128))\n","\n","    # Dec_Out_1 1: IMG 128x128, FEAT 144 -> 2\n","    self.cells.append(BackBoneCell(144 * n, 2, 128, 128))\n","\n","  def forward(self, inp):\n","\n","    # Encoder Branch\n","    enc1 = self.cells[0](inp)\n","    enc2 = self.cells[1](enc1)\n","    enc3 = self.cells[2](enc2)\n","    enc4 = self.cells[3](enc3)\n","    enc5 = self.cells[4](enc4)\n","\n","    # Decoder 4\n","    skip_1_4 = self.cells[5](enc1)\n","    skip_2_4 = self.cells[6](enc2)\n","    skip_3_4 = self.cells[7](enc3)\n","    skip_4_4 = self.cells[8](enc4)\n","\n","    enc_dec_5_4 = self.cells[9](enc5)\n","\n","    dec_4 = self.cells[10](\n","        torch.cat((\n","            skip_1_4,\n","            skip_2_4,\n","            skip_3_4,\n","            skip_4_4,\n","            enc_dec_5_4\n","            ), dim = 1)\n","        )\n","\n","    # Decoder 3\n","    skip_1_3 = self.cells[11](enc1)\n","    skip_2_3 = self.cells[12](enc2)\n","    skip_3_3 = self.cells[13](enc3)\n","    skip_4_3 = self.cells[14](enc4)\n","\n","    dec_4_3 = self.cells[15](dec_4)\n","\n","    dec_3 = self.cells[16](\n","        torch.cat((\n","            skip_1_3,\n","            skip_2_3,\n","            skip_3_3,\n","            skip_4_3,\n","            dec_4_3\n","            ), dim = 1)\n","        )\n","\n","    # Decoder 2\n","    skip_1_2 = self.cells[17](enc1)\n","    skip_2_2 = self.cells[18](enc2)\n","    skip_3_2 = self.cells[19](enc3)\n","    skip_4_2 = self.cells[20](enc4)\n","\n","    dec_3_2 = self.cells[21](dec_3)\n","\n","    dec_2 = self.cells[22](\n","        torch.cat((\n","            skip_1_2,\n","            skip_2_2,\n","            skip_3_2,\n","            skip_4_2,\n","            dec_3_2\n","            ), dim = 1)\n","        )\n","\n","    # Decoder 1\n","    skip_1_1 = self.cells[23](enc1)\n","    skip_2_1= self.cells[24](enc2)\n","    skip_3_1 = self.cells[25](enc3)\n","    skip_4_1 = self.cells[26](enc4)\n","\n","    dec_2_1 = self.cells[27](dec_2)\n","\n","    dec_1 = self.cells[28](\n","        torch.cat((\n","            skip_1_1,\n","            skip_2_1,\n","            skip_3_1,\n","            skip_4_1,\n","            dec_2_1\n","        ), dim = 1))\n","\n","    return dec_1"],"metadata":{"id":"JXkg9sxTh1ck","executionInfo":{"status":"ok","timestamp":1715414298343,"user_tz":420,"elapsed":164,"user":{"displayName":"Christian Granados","userId":"18308979293956140005"}}},"execution_count":64,"outputs":[]},{"cell_type":"markdown","source":["### 9.4 Discrete Training & Evaluation"],"metadata":{"id":"kAL4-dWfcj71"}},{"cell_type":"code","source":["# Discrete Logging Dataframe\n","dcolumns_ = [\n","    \"Time\", \"Epoch\", \"Step\", \"LR\",\n","    \"Loss\", \"Accuracy\", \"IoU\", \"Dice\"\n","    ]\n","drow_list = []"],"metadata":{"id":"t4L7EdylC0w0","executionInfo":{"status":"ok","timestamp":1715413861160,"user_tz":420,"elapsed":121,"user":{"displayName":"Christian Granados","userId":"18308979293956140005"}}},"execution_count":54,"outputs":[]},{"cell_type":"code","source":["# Load Data\n","data_len = 2560\n","train_data = HAM10000_Dataset(\"/content/drive/MyDrive/Granados_Thesis_SP24/HAM10000/image_indexing.csv\", num_entries = data_len)\n","\n","# 47.5% Train / 47.5% Validation/ 5% Test\n","indices = list(range(data_len))\n","train_val_split = int(np.floor(0.475 * len(train_data)))\n","val_test_split = int(np.floor(0.95 * len(train_data)))\n","\n","train_queue = torch.utils.data.DataLoader(\n","    train_data,\n","    batch_size = batch_size,\n","    sampler = torch.utils.data.sampler.SubsetRandomSampler(indices[:train_val_split]),\n","    pin_memory = True,\n","    drop_last = True\n","    )\n","\n","val_queue = torch.utils.data.DataLoader(\n","    train_data,\n","    batch_size = batch_size,\n","    sampler = torch.utils.data.sampler.SubsetRandomSampler(indices[train_val_split:val_test_split]),\n","    pin_memory = True,\n","    drop_last = True\n","    )\n","\n","test_queue = torch.utils.data.DataLoader(\n","    train_data,\n","    batch_size = batch_size,\n","    sampler = torch.utils.data.sampler.SubsetRandomSampler(indices[val_test_split:]),\n","    pin_memory = True,\n","    drop_last = True\n","    )"],"metadata":{"id":"g6RJuWwpSkTn","executionInfo":{"status":"ok","timestamp":1715413862599,"user_tz":420,"elapsed":123,"user":{"displayName":"Christian Granados","userId":"18308979293956140005"}}},"execution_count":55,"outputs":[]},{"cell_type":"code","source":["epochs = 40\n","\n","torch.cuda.empty_cache()\n","if not torch.cuda.is_available():\n","  print(\"No GPU Device Availible\")\n","\n","# Model\n","discrete_model = DiscreteNetwork(finalnet_node_tuples, finalnet_csb_tuples, finalnet_ssb_tuples, n = 2).cuda()\n","\n","# Number of Parameters\n","params = sum(p.numel() for p in discrete_model.parameters() if p.requires_grad)\n","print(\"Total Parameters:\", params)\n","\n","# Loss\n","criterion = nn.BCEWithLogitsLoss().cuda()\n","\n","# Optimizers\n","optimizer = torch.optim.RAdam(discrete_model.parameters(), lr = lr, betas = (0.5, 0.999), weight_decay = 1e-4, decoupled_weight_decay = True)\n","\n","# Scheduler\n","scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, float(epochs), eta_min = lr_min)\n","\n","# Train (Inter-Epoch)\n","for epoch in range(epochs):\n","    lr = scheduler.get_lr()[0]\n","    print(f\"epoch {epoch} lr {lr}\")\n","\n","    objs_ = AvgrageMeter()\n","    acc_ = AvgrageMeter()\n","    iou_ = AvgrageMeter()\n","    dice_ = AvgrageMeter()\n","\n","    for step, ((inp, tar), (inp_val, tar_val)) in enumerate(zip(train_queue, val_queue)):\n","\n","        discrete_model.train()\n","        n = inp.size(0)\n","\n","        inp = inp.cuda()\n","        tar = tar.cuda(non_blocking=True)\n","\n","        optimizer.zero_grad()\n","        logits = discrete_model(inp)\n","        loss = criterion(logits, tar)\n","        loss.mean().backward()\n","        nn.utils.clip_grad_norm_(discrete_model.parameters(), grad_clip)\n","        optimizer.step()\n","\n","        objs_.update(loss.item())\n","\n","        discrete_model.eval()\n","        with torch.no_grad():\n","          inp_val = inp_val.cuda()\n","          tar_val = tar_val.cuda(non_blocking=True)\n","\n","          logits_ = discrete_model(inp_val)\n","\n","          acc_.update(accuracy(logits_, tar_val), n = batch_size)\n","          iou_.update(iou(logits_, tar_val), n = batch_size)\n","          dice_.update(dice(logits_, tar_val), n = batch_size)\n","\n","        if step % rep_freq == 0:\n","            print(\"Training Step:\", step,\n","                  \"Loss:\", objs_.getAverage(),\n","                  \"Accuracy\", acc_.getAverage(),\n","                  \"IoU\", iou_.getAverage(),\n","                  \"Dice\", dice_.getAverage()\n","                  )\n","            drow_list.append({\n","                \"Time\": datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\"),\n","                \"Epoch\": epoch,\n","                \"Step\": step,\n","                \"LR\": lr,\n","                \"Loss\": objs_.getAverage(),\n","                \"Accuracy\": acc_.getAverage(),\n","                \"IoU\": iou_.getAverage(),\n","                \"Dice\": dice_.getAverage()\n","            })\n","\n","\n","    with torch.no_grad():\n","\n","        if epoch == epochs - 1:\n","\n","            test_acc, test_iou, test_dice = infer(test_queue, discrete_model, criterion)\n","            print(\"Test Accuracy:\", test_acc, \"Test IoU:\", test_iou, \"Test Dice:\", test_dice)\n","            drow_list.append({\n","                \"Time\": datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\"),\n","                \"Epoch\": epoch,\n","                \"Step\": -1,\n","                \"LR\": lr,\n","                \"Loss\": -1,\n","                \"Accuracy\": test_acc,\n","                \"IoU\": test_iou,\n","                \"Dice\": test_dice\n","            })\n","\n","    scheduler.step()\n","\n","# Assemble & Save log\n","discrete_log_df = pd.DataFrame(drow_list, columns = dcolumns_)\n","discrete_log_df.to_csv('/content/drive/MyDrive/Granados_Thesis_SP24/discrete_log_df.csv')"],"metadata":{"id":"mSTiZcjjh15w"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 10. Results\n","\n","DARTs-UNET Search Phase"],"metadata":{"id":"1g0KS4jLByxY"}},{"cell_type":"code","source":["log_df = pd.read_csv('/content/drive/MyDrive/Granados_Thesis_SP24/log_df.csv').drop(columns = [\"Unnamed: 0\"])"],"metadata":{"id":"-d6NjYJXGfbH","executionInfo":{"status":"ok","timestamp":1715338058952,"user_tz":420,"elapsed":365,"user":{"displayName":"Christian Granados","userId":"18308979293956140005"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["loss_overtime = log_df[log_df[\"Loss\"] >= 0]\n","loss_overtime.loc[:, \"epoch_step\"] = loss_overtime[\"Epoch\"] + (loss_overtime[\"Step\"] / 64)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VkCTEKPKHEZ6","executionInfo":{"status":"ok","timestamp":1715340051892,"user_tz":420,"elapsed":444,"user":{"displayName":"Christian Granados","userId":"18308979293956140005"}},"outputId":"93924b24-5b3e-4544-c4ba-9cba67ceed12"},"execution_count":52,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-52-b5f81beaa90e>:2: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  loss_overtime.loc[:, \"epoch_step\"] = loss_overtime[\"Epoch\"] + (loss_overtime[\"Step\"] / 64)\n"]}]},{"cell_type":"code","source":["plt.title(\"Network Loss Overtime\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"BCELogitLoss\")\n","plt.xticks(np.arange(0, 31, 2))\n","plt.grid(color = 'grey', linestyle = '--', linewidth = 1, alpha = 0.3)\n","plt.plot(loss_overtime[\"epoch_step\"], loss_overtime[\"Loss\"])\n","plt.scatter(loss_overtime[\"epoch_step\"][::40], loss_overtime[\"Loss\"][::40])\n","for x, y in zip(loss_overtime[\"epoch_step\"][::40], loss_overtime[\"Loss\"][::40]):\n","    plt.text(x + 0.4, y,\n","        s = \"Loss: \" + str(round(y, 3)),\n","        bbox = dict(facecolor = 'grey', edgecolor = 'none', boxstyle = 'round,pad=0.05', alpha = 0.2),\n","        rotation = 15,\n","        horizontalalignment = \"left\",\n","        verticalalignment = \"bottom\",\n","        rotation_mode = \"anchor\",\n","        )\n","plt.show()"],"metadata":{"id":"PgDK36EeIYjg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.title(\"Network Accuracy Overtime\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Accuracy\")\n","plt.xticks(np.arange(0, 31, 2))\n","plt.grid(color = 'grey', linestyle = '--', linewidth = 1, alpha = 0.3)\n","plt.plot(loss_overtime[\"epoch_step\"], loss_overtime[\"Accuracy\"])\n","plt.scatter(loss_overtime[\"epoch_step\"][::40], loss_overtime[\"Accuracy\"][::40])\n","for x, y in zip(loss_overtime[\"epoch_step\"][::40], loss_overtime[\"Accuracy\"][::40]):\n","    plt.text(x + 0.4, y - 0.01,\n","        s = \"Acc: \" + str(round(y, 3)),\n","        bbox = dict(facecolor = 'grey', edgecolor = 'none', boxstyle = 'round,pad=0.05', alpha = 0.2),\n","        rotation = -30,\n","        horizontalalignment = \"left\",\n","        verticalalignment = \"bottom\",\n","        rotation_mode = \"anchor\",\n","        )\n","plt.show()"],"metadata":{"id":"ZFINzvqrH1RP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.title(\"Network IoU Overtime\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"IoU\")\n","plt.xticks(np.arange(0, 31, 2))\n","plt.grid(color = 'grey', linestyle = '--', linewidth = 1, alpha = 0.3)\n","plt.plot(loss_overtime[\"epoch_step\"], loss_overtime[\"IoU\"])\n","plt.scatter(loss_overtime[\"epoch_step\"][::40], loss_overtime[\"IoU\"][::40])\n","for x, y in zip(loss_overtime[\"epoch_step\"][::40], loss_overtime[\"IoU\"][::40]):\n","    plt.text(x + 0.4, y - 0.01,\n","        s = \"IoU: \" + str(round(y, 3)),\n","        bbox = dict(facecolor = 'grey', edgecolor = 'none', boxstyle = 'round,pad=0.05', alpha = 0.2),\n","        rotation = -30,\n","        horizontalalignment = \"left\",\n","        verticalalignment = \"bottom\",\n","        rotation_mode = \"anchor\",\n","        )\n","plt.show()"],"metadata":{"id":"aO3vEBDYNayt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.title(\"Network Dice Overtime\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Dice\")\n","plt.xticks(np.arange(0, 31, 2))\n","plt.grid(color = 'grey', linestyle = '--', linewidth = 1, alpha = 0.3)\n","plt.plot(loss_overtime[\"epoch_step\"], loss_overtime[\"Dice\"])\n","plt.scatter(loss_overtime[\"epoch_step\"][::40], loss_overtime[\"Dice\"][::40])\n","for x, y in zip(loss_overtime[\"epoch_step\"][::40], loss_overtime[\"Dice\"][::40]):\n","    plt.text(x + 0.4, y - 0.01,\n","        s = \"Dice: \" + str(round(y, 3)),\n","        bbox = dict(facecolor = 'grey', edgecolor = 'none', boxstyle = 'round,pad=0.05', alpha = 0.2),\n","        rotation = -30,\n","        horizontalalignment = \"left\",\n","        verticalalignment = \"bottom\",\n","        rotation_mode = \"anchor\",\n","        )\n","plt.show()"],"metadata":{"id":"2nSqV1AWNkzW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["alphas_df = log_df[log_df[\"SSB_A\"].apply(lambda x: x != \"[]\")]"],"metadata":{"id":"X8ql0iOBPFeO","executionInfo":{"status":"ok","timestamp":1715340376001,"user_tz":420,"elapsed":2,"user":{"displayName":"Christian Granados","userId":"18308979293956140005"}}},"execution_count":74,"outputs":[]},{"cell_type":"code","source":["tmp_lst = list(map(lambda x : ast.literal_eval(x), alphas_df[\"SSB_A\"].tolist()))[::16]\n","tmp_df = pd.DataFrame(tmp_lst, columns = [\"Node 0\", \"Node 1\", \"Node 2\", \"Node 3\"])\n","tmp_df[\"epoch\"] = pd.Series(np.arange(30))\n","\n","plt.title(\"Skip_1_4 SSB Alphas Overtime\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Connection Strength\")\n","plt.xticks(np.arange(0, 30, 2))\n","plt.grid(color = 'grey', linestyle = '--', linewidth = 1, alpha = 0.3)\n","for c in tmp_df.columns[:-1]:\n","    plt.plot(tmp_df[\"epoch\"], tmp_df[c], label = c)\n","\n","plt.legend()\n","plt.show()"],"metadata":{"id":"Iwxb8yUUPRL6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tmp_lst = list(map(lambda x : ast.literal_eval(x), alphas_df[\"SSB_A\"].tolist()))[10::16]\n","tmp_df = pd.DataFrame(tmp_lst, columns = [\"Node 0\", \"Node 1\", \"Node 2\", \"Node 3\"])\n","tmp_df[\"epoch\"] = pd.Series(np.arange(30))\n","\n","plt.title(\"Skip_3_2 SSB Alphas Overtime\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Connection Strength\")\n","plt.xticks(np.arange(0, 30, 2))\n","plt.grid(color = 'grey', linestyle = '--', linewidth = 1, alpha = 0.3)\n","for c in tmp_df.columns[:-1]:\n","    plt.plot(tmp_df[\"epoch\"], tmp_df[c], label = c)\n","\n","plt.legend()\n","plt.show()"],"metadata":{"id":"saJaqZgYS72b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tmp_lst = list(map(lambda x : ast.literal_eval(x), alphas_df[\"CSB_A\"].tolist()))[::16]\n","tmp_df = pd.DataFrame(tmp_lst, columns = [\"Node 0\", \"Node 1\", \"Node 2\", \"Node 3\"])\n","tmp_df[\"epoch\"] = pd.Series(np.arange(30))\n","\n","plt.title(\"Skip_1_4 CSB Alphas Overtime\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Connection Strength\")\n","plt.xticks(np.arange(0, 30, 2))\n","plt.grid(color = 'grey', linestyle = '--', linewidth = 1, alpha = 0.3)\n","for c in tmp_df.columns[:-1]:\n","    plt.plot(tmp_df[\"epoch\"], tmp_df[c], label = c)\n","\n","plt.legend()\n","plt.show()"],"metadata":{"id":"jUftp9QDU7-C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tmp_lst = list(map(lambda x : ast.literal_eval(x), alphas_df[\"CSB_A\"].tolist()))[10::16]\n","tmp_df = pd.DataFrame(tmp_lst, columns = [\"Node 0\", \"Node 1\", \"Node 2\", \"Node 3\"])\n","tmp_df[\"epoch\"] = pd.Series(np.arange(30))\n","\n","plt.title(\"Skip_3_2 CSB Alphas Overtime\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Connection Strength\")\n","plt.xticks(np.arange(0, 30, 2))\n","plt.grid(color = 'grey', linestyle = '--', linewidth = 1, alpha = 0.3)\n","for c in tmp_df.columns[:-1]:\n","    plt.plot(tmp_df[\"epoch\"], tmp_df[c], label = c)\n","\n","plt.legend()\n","plt.show()"],"metadata":{"id":"gO7dnEj0VGaQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tmp_lst = list(map(lambda x : ast.literal_eval(x), alphas_df[\"Node_1_A\"].tolist()))[::16]\n","tmp_df = pd.DataFrame(tmp_lst, columns = PRIMITIVES)\n","tmp_df[\"epoch\"] = pd.Series(np.arange(30))\n","\n","plt.title(\"Skip_1_4 Node_1 Alphas Overtime\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Connection Strength\")\n","plt.xticks(np.arange(0, 30, 2))\n","plt.grid(color = 'grey', linestyle = '--', linewidth = 1, alpha = 0.3)\n","for c in tmp_df.columns[:-1]:\n","    plt.plot(tmp_df[\"epoch\"], tmp_df[c], label = c)\n","\n","plt.legend()\n","plt.show()"],"metadata":{"id":"vlIdVzdvZkNo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tmp_lst = list(map(lambda x : ast.literal_eval(x), alphas_df[\"Node_1_A\"].tolist()))[10::16]\n","tmp_df = pd.DataFrame(tmp_lst, columns = PRIMITIVES)\n","tmp_df[\"epoch\"] = pd.Series(np.arange(30))\n","\n","plt.title(\"Skip_3_2 Node_1 Alphas Overtime\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Connection Strength\")\n","plt.xticks(np.arange(0, 30, 2))\n","plt.grid(color = 'grey', linestyle = '--', linewidth = 1, alpha = 0.3)\n","for c in tmp_df.columns[:-1]:\n","    plt.plot(tmp_df[\"epoch\"], tmp_df[c], label = c)\n","\n","plt.legend()\n","plt.show()"],"metadata":{"id":"rMjTsSwfaE5F"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Discretization Phase"],"metadata":{"id":"QUeMZvzlcQwB"}}]}